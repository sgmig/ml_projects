{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial keypoints detection Kaggle Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my attempt at using ML to complete the taks proposed by the challenge. \n",
    "\n",
    "The dataset consists of 96x96px images of faces, and the goal is to find the position of 15 features in each image. \n",
    "\n",
    "The train sets containts the pictures and the 30 (x,y) coordinates of the features for each training example. \n",
    "\n",
    "I'll begin by visualizing some of the pictures, and if possible their features. Then I'll think about how to detect the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training set\n",
    "\n",
    "source_data = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(source_data['Image'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pictures are given as strings. I will need to convert them to lists of integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all the pictures\n",
    "\n",
    "photos = source_data['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))\n",
    "\n",
    "# Now I have the pictures as arrays of pixel intensity.\n",
    "\n",
    "np.sqrt(len(photos[0])) # Checking that the arrays have the right size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(photos.head()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting a face.\n",
    "plt.imshow(random.choice(photos).reshape(96,96),cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting some random faces to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_size = 4\n",
    "chosen_faces = random.sample(range(0,len(photos)), grid_size**2) #picking faces for a 4x4 grid. \n",
    "\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "for i, ax in zip(chosen_faces, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.imshow(photos[i].reshape(96,96),cmap='gist_gray')\n",
    "    \n",
    "#plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the features\n",
    "\n",
    "Now that we have the faces, lets have a closer look at the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = source_data.drop('Image', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keypoints.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7041 is missing features. Lets plot it to see what that looks like.\n",
    "\n",
    "#Getting the keypoints positions\n",
    "guy_face = keypoints.iloc[7041]\n",
    "\n",
    "x_points = [guy_face[i] for i in range(0,30,2)]\n",
    "y_points = [guy_face[i+1] for i in range(0,30,2)] \n",
    "\n",
    "# Plotting a face.\n",
    "\n",
    "plt.imshow(photos[7041].reshape(96,96),cmap='gist_gray')\n",
    "plt.plot(x_points, y_points, 'ro', markerfacecolor = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting faces with features\n",
    "\n",
    "grid_size = 4\n",
    "chosen_faces = random.sample(range(0,len(photos)), grid_size**2) #picking faces for a 4x4 grid. \n",
    "\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "for i, ax in zip(chosen_faces, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.imshow(photos[i].reshape(96,96),cmap='gist_gray')\n",
    "    \n",
    "    x_points = [keypoints.iloc[i][j] for j in range(0,30,2)]\n",
    "    y_points = [keypoints.iloc[i][j+1] for j in range(0,30,2)] \n",
    "    \n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving new dataframes with photos and keypoints to new csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works, but you end up with a csv file of about 2gb. It is just not worth it nor necessary.\n",
    "# My only reason for doing this was that I needed to use this on a different computer. \n",
    "\n",
    "\"\"\"photos_for_csv = source_data['Image'].apply(lambda str_pic: [int(px) for px in str_pic.split()])\n",
    "\n",
    "photo_arr = np.array([l for l in photos_for_csv])\n",
    "\n",
    "np.savetxt('training_photos.csv', photo_arr, delimiter = ',')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"photos.to_csv(r'training_photos.csv')\n",
    "keypoints.to_csv(r'training_keypoints.csv')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dealing with missing labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to eventually put all of these train examples into a neural network, but first I need to deal with some issues. \n",
    "\n",
    "* MANY of the pictures are lacking some keypoints positions. This poses the problem of what the algorithm should find for those examples. I can think of a few alternatives:\n",
    "\n",
    "    * Try to complete the missing labels and then use everything to train a NN that gives all 15 keypoints at once:\n",
    "    \n",
    "        * By manually finding and labelling the missing keypoints --> Of course not. \n",
    "        \n",
    "        * Use the known labels as _features_, and train some sort of algorithm to find the rest of them. I could use KNN, for example, and fill in the missing info. I think it shouldn't take too long to give this a try. The problem is that I need to find batches of images having the same labels, to use as the Nearest Neighbors. So this would mean using vectors with different dimensions, or choosing at each step which features to ignore and to use as reference. __This sort of defeats the purpose: I would build a ML algorithm to find keypoints, and then use those keypoints to train a ML algorithm to find keypoints. The difference is that in the first case the feaures are known keypoints, while in the second they are the pixel intensities of the image, but still...__\n",
    "        \n",
    "        * Input the missing labels using the mean position of each feature, or by finding the distribution of positions for that feature and drawing randomly from it. \n",
    "    \n",
    "    For all these methods of inputing the missing data I can sort of evaluate the result by looking at pictures with inputed keypoints, and see if the filled-in positions look more or less right. However, this does not sound very rigurous. \n",
    "    \n",
    "    * If I don't want to fill in the missing labels, I could train one NN for each specific keypoints, and feed as trianing only images where the said keypoint is known. This implies training 15 different NNs, with a different number of training examples each. \n",
    "    \n",
    "        * I could use these set of networks to predict the keypoints I need to give as a solution, or use them to complete the missing points, and then retrain. This souns more iterative and somewhat suboptimal. __Also quite redundant, and I'd probably be introducing some bias this way__.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding images with the same labeled keypoints. \n",
    "\n",
    "I want to make a table telling me which keypoints are already labelled for each training images. I guess veryfing that only one coordinate for the feature is enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new columns names by removing the subindices. \n",
    "new_col_list = [keypoints.columns[i][:-2] for i in range(0,len(keypoints.columns),2)]\n",
    "new_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dataframe saying which keypoints are marked on each image. \n",
    "present_keypoints = pd.DataFrame(columns = new_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_col_list:\n",
    "    present_keypoints[col] = pd.notnull(keypoints[col+'_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_keypoints['Total'] = present_keypoints.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_keypoints['Total'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(binary_df[binary_df['Total'] == 14].drop('Total', axis = 1), yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that most images have only four keypoints labelled, and that furthermore, for images with the same number of keypoints, the missings ones usually do no coincide. I could try and see which are the more and lees common missing points, but it is probably not very important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Filling in missing keypoints\n",
    "\n",
    "I'll try too see how does it look when I fill the missing labels with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_positions = keypoints.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting faces with features\n",
    "\n",
    "grid_size = 4\n",
    "chosen_faces = random.sample(range(0,len(photos)), grid_size**2) #picking faces for a 4x4 grid. \n",
    "\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "for i, ax in zip(chosen_faces, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.imshow(photos[i].reshape(96,96), cmap='gist_gray')\n",
    "    \n",
    "    x_points = [keypoints.iloc[i][j] for j in range(0,30,2)]\n",
    "    y_points = [keypoints.iloc[i][j+1] for j in range(0,30,2)] \n",
    "    \n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')\n",
    "    \n",
    "    filled_x = [keypoints.mean()[j] if np.isnan(x_points[int(j/2)]) else np.nan for j in range(0,30,2) ]\n",
    "    filled_y = [keypoints.mean()[j+1] if np.isnan(y_points[int(j/2)]) else np.nan for j in range(0,30,2) ]\n",
    "    \n",
    "    ax.plot(filled_x, filled_y, 'bD', markerfacecolor = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a df with complementary keypoints.\n",
    "\n",
    "complement_keypoints = pd.DataFrame(columns = keypoints.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keypoints[pd.notnull(keypoints)].iloc[7041] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4 - np.nan ) * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keypoints[pd.notnull(keypoints)].iloc[7041] )):\n",
    "    if np.isnan(keypoints[pd.notnull(keypoints)].iloc[7041][i] ):\n",
    "        print('replace')\n",
    "    else:\n",
    "        print('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Missing labels and Tensorflow\n",
    "\n",
    "My goal is to train a CNN to identify the keypoints. I need to check wheter the loss function included in tensorflow can handle the missing labels. \n",
    "\n",
    "I will create some random predictions, including some Nan values on the real labels, and see what tensorflow does. I think I will need to include some weights to remove the coordinates for which I have no predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.arange(5) + (2*np.random.rand(5) - 1) # Randome values for my predictions.\n",
    "y_true = np.arange(5) * 1.0 # true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(5))\n",
    "plt.plot(np.arange(5), y_preds, 'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's put some nan values\n",
    "y_true[[2,4] ]= np.nan\n",
    "\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(5))\n",
    "plt.plot(np.arange(5), y_true, 'bo')\n",
    "plt.plot(np.arange(5), y_preds, 'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDERS for the loss function\n",
    "preds = tf.placeholder(tf.float32, [None, 1])\n",
    "full_true = tf.placeholder(tf.float32, [None, 1])\n",
    "true = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "weights = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now let's creat a tensorflow thing that computes the mean squared error.\n",
    "# I could use the argument weigths to ignore the terms that dont have labels, provided it multiplies by zero. \n",
    "\n",
    "full_error = tf.losses.mean_squared_error(preds, full_true)\n",
    "nan_error = tf.losses.mean_squared_error(preds, true)\n",
    "nan_error_weights = tf.losses.mean_squared_error(preds, true, weights )\n",
    "\n",
    "# I found the solution! I need a boolean mask!\n",
    "\n",
    "boolean_error = tf.reduce_mean(tf.square(tf.boolean_mask(preds - true, tf.is_finite(true))))\n",
    "mean_err_mask = tf.losses.mean_squared_error(tf.boolean_mask(preds, tf.is_finite(true)), \n",
    "                                             tf.boolean_mask(true, tf.is_finite(true)) \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the values\n",
    "\n",
    "y_preds = (np.arange(5) + (2*np.random.rand(5) - 1)).reshape((-1,1)) # Randome values for my predictions.\n",
    "full_y_true = (np.arange(5) * 1.0).reshape((-1,1))\n",
    "\n",
    "y_true = np.arange(5) * 1.0 # true values\n",
    "y_true[[2,4] ]= np.nan\n",
    "y_true = y_true.reshape((-1,1))\n",
    "\n",
    "weight_vals = np.isfinite(y_true) *1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    test = sess.run(tf.square(tf.boolean_mask(preds - true, tf.is_finite(preds-true))), \n",
    "             feed_dict={preds:y_preds, true:y_true})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    full = sess.run(full_error, feed_dict = {preds:y_preds, full_true:full_y_true } )\n",
    "    nan_e = sess.run(nan_error, feed_dict = {preds:y_preds, true:y_true } )\n",
    "    nan_e_weights = sess.run(nan_error, feed_dict = {preds:y_preds, true:y_true, weights:weight_vals } )\n",
    "    bool_err = sess.run(boolean_error, feed_dict = {preds:y_preds, true:y_true } )\n",
    "    full_bool_err = sess.run(boolean_error, feed_dict = {preds:y_preds, true:full_y_true } )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(nan_e, nan_e_weights, full, full_bool_err, bool_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which way is faster\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    bool_err = sess.run(boolean_error, feed_dict = {preds:y_preds, true:y_true } )\n",
    "    \n",
    "duration = time.time() - start_time\n",
    "print('Time using custom function ', duration)\n",
    "print('Custom error: ', bool_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    bool_err = sess.run(mean_err_mask, feed_dict = {preds:y_preds, true:y_true } )\n",
    "    \n",
    "duration = time.time() - start_time\n",
    "print('Time using built in mse', duration)\n",
    "print('Built in mse: ', bool_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the custom function is faster! It makes sense, because I am only applying the mask once. \n",
    "\n",
    "I think I have everything I need to try to implement the model. Maybe it is more practical to do this in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
