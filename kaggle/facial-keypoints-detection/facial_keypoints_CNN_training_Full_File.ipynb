{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional NN it for facial keypoints recognition\n",
    "\n",
    "Here I will build a convolutional neural network, and train it for the task of facial keypoints recognition. The data are obtained from Kaggle: **LINK**, and consists of **info**.\n",
    "\n",
    "I will build the CNN using tensorflow **link**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_full = pd.read_csv('training.csv')\n",
    "#training_data.info()\n",
    "\n",
    "# I will do the splitting here, so that I have a test set that has never been passed \n",
    "# through the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and cross-validation sets. \n",
    "training_data, training_cv = train_test_split(training_data_full,\n",
    "                                              test_size = 50, random_state = 42) #set the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = training_data_full['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "images = np.vstack(images.iloc[i] for i in range(len(images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = training_data_full.drop('Image', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cv, keypoints_train, keypoints_cv = train_test_split(images, keypoints, \n",
    "                                                                test_size=100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size):\n",
    "    \n",
    "    sample_indices = np.random.randint(len(x_train), size = batch_size)\n",
    "    \n",
    "    images = x_train[sample_indices]\n",
    "    keypoints = keypoints_train.iloc[sample_indices]\n",
    "        \n",
    "    return images, keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN\n",
    "\n",
    "I will start by using the same architechture I used in the course.For the moment I will use train_test_split to test my network a bit. Eventually this wont be necessary, as the dataset provides a separate test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some helping functions.\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.random_normal(shape, stddev=0.1) # Why this stddev?\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.random_uniform(shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 4, 4, 1],\n",
    "                          strides=[1, 4, 4, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDERS\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 9216])\n",
    "keypoints_true = tf.placeholder(tf.float32, [None, 30])\n",
    "lr = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_images = tf.reshape(x, [-1,96,96,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERS\n",
    "\n",
    "convo1 = convolutional_layer(x_images, [8,8,1,32]) # 8 x 8 filter, 1 channel in, 32 channels out. SAME padding.\n",
    "                                            # so output images are also 96 x 96. \n",
    "convo1_pool = max_pool_2by2(convo1)   #output of 24 x 24 x 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo2 = convolutional_layer(convo1_pool, [4,4,32,64]) # 4x4 filter, 64 outputs. SAME padding.\n",
    "convo2_pool = max_pool_2by2(convo2) # 6 x 6 x 64\n",
    "\n",
    "convo2_flat = tf.reshape(convo2_pool,[-1,6*6*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo2_flat,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"convo3 = convolutional_layer(convo2_pool, [4,4,64,128]) # 4x4 filter, 64 outputs. SAME padding.\n",
    "convo3_pool = max_pool_2by2(convo3) # 12 x 12 x 128\n",
    "convo3_flat = tf.reshape(convo3_pool,[-1,12*12*128])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo3_flat,1024))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPOUT AND OUTPOUT LAYER\n",
    "drop_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one, rate = drop_prob)\n",
    "\n",
    "keypoints_pred = normal_full_layer(full_one_dropout,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "\n",
    "masked_loss = tf.reduce_mean(tf.square(\n",
    "        tf.boolean_mask(keypoints_pred-keypoints_true, tf.is_finite(keypoints_true) )\n",
    "    ))\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train = optimizer.minimize(masked_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZER\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_mean = keypoints_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.69596922581684"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing error from the mean in order to have a reference. This is the mark I have to beat. \n",
    "\n",
    "mean_keypoints_mse = ((keypoints_cv-keypoints_mean)**2).sum(axis = 1).mean()\n",
    "mean_keypoints_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on step 0\n",
      "Train MSE: \n",
      "8248813.0 \n",
      "\n",
      "CV MSE:\n",
      "1223629.2\n",
      "\n",
      "\n",
      "train_loss: \n",
      "[8248813.0]\n",
      "cv_loss: \n",
      "[1223629.2] \n",
      "\n",
      "Currently on step 100\n",
      "Train MSE: \n",
      "5215.374 \n",
      "\n",
      "CV MSE:\n",
      "2398.4114\n",
      "\n",
      "\n",
      "train_loss: \n",
      "[8248813.0, 5215.374]\n",
      "cv_loss: \n",
      "[1223629.2, 2398.4114] \n",
      "\n",
      "Currently on step 200\n",
      "Train MSE: \n",
      "1627.257 \n",
      "\n",
      "CV MSE:\n",
      "852.15576\n",
      "\n",
      "\n",
      "train_loss: \n",
      "[8248813.0, 5215.374, 1627.257]\n",
      "cv_loss: \n",
      "[1223629.2, 2398.4114, 852.15576] \n",
      "\n",
      "Currently on step 300\n",
      "Train MSE: \n",
      "963.4868 \n",
      "\n",
      "CV MSE:\n",
      "402.28214\n",
      "\n",
      "\n",
      "train_loss: \n",
      "[8248813.0, 5215.374, 1627.257, 963.4868]\n",
      "cv_loss: \n",
      "[1223629.2, 2398.4114, 852.15576, 402.28214] \n",
      "\n",
      "Currently on step 400\n",
      "Train MSE: \n",
      "538.09033 \n",
      "\n",
      "CV MSE:\n",
      "337.3574\n",
      "\n",
      "\n",
      "train_loss: \n",
      "[8248813.0, 5215.374, 1627.257, 963.4868, 538.09033]\n",
      "cv_loss: \n",
      "[1223629.2, 2398.4114, 852.15576, 402.28214, 337.3574] \n",
      "\n",
      "Currently on step 500\n",
      "Train MSE: \n",
      "696.5594 \n",
      "\n",
      "CV MSE:\n",
      "241.59712\n",
      "\n",
      "\n",
      "train_loss: \n",
      "[8248813.0, 5215.374, 1627.257, 963.4868, 538.09033, 696.5594]\n",
      "cv_loss: \n",
      "[1223629.2, 2398.4114, 852.15576, 402.28214, 337.3574, 241.59712] \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-c769899af8bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         _ , train_loss = sess.run([train, masked_loss], \n\u001b[0;32m     14\u001b[0m                                   feed_dict={x: x_batch, keypoints_true: keypoints_batch,\n\u001b[1;32m---> 15\u001b[1;33m                                              drop_prob: 0.5, lr:0.0005})\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# PRINT OUT A MESSAGE EVERY 100 STEPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_cnn_3\")\n",
    "    train_losses = []\n",
    "    cv_losses = []\n",
    "    for iteration in range(num_steps+1):\n",
    "        \n",
    "        x_batch, keypoints_batch = next_batch(50)\n",
    "        \n",
    "        _ , train_loss = sess.run([train, masked_loss], \n",
    "                                  feed_dict={x: x_batch, keypoints_true: keypoints_batch,\n",
    "                                             drop_prob: 0.5, lr:0.0005})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        \n",
    "        #if (iteration%20 == 0) and (iteration >0):\n",
    "            #print(':')\n",
    "        \n",
    "        if iteration%100 == 0:\n",
    "            \n",
    "            cv_loss = sess.run(masked_loss,feed_dict={x:x_cv,keypoints_true:keypoints_cv,\n",
    "                                                      drop_prob:0})\n",
    "            \n",
    "            print('Currently on step {}'.format(iteration))\n",
    "            print('Train MSE: ')\n",
    "            print(train_loss, '\\n')\n",
    "            print('CV MSE:')\n",
    "            # Test the Train Model\n",
    "            print(cv_loss)\n",
    "            print('\\n')\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            cv_losses.append(cv_loss)\n",
    "            print('train_loss: ')\n",
    "            print( train_losses)\n",
    "            print('cv_loss: ')\n",
    "            print(cv_losses, '\\n')\n",
    "            \n",
    "    \n",
    "    saver.save(sess, \"./saved_models/keypoints_cnn_\" + str(counter) )\n",
    "    #saver.save(sess, \"./saved_models/keypoints_cnn_3\"  )\n",
    "    #counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Use your Saver instance to restore your saved rnn time series model\n",
    "    saver.restore(sess, \"./saved_models/keypoints_cnn_1\")\n",
    "\n",
    "    # Create a numpy array for your genreative seed from the last 12 months of the \n",
    "    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n",
    "        \n",
    "    predictions = sess.run(keypoints_pred, feed_dict= {x:x_cv_small, hold_prob:1.0 })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "mean_x_points = [keypoints_mean[j] for j in range(0,30,2)]\n",
    "mean_y_points = [keypoints_mean[j+1] for j in range(0,30,2)]\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    # Plotting the faces\n",
    "    ax.imshow(x_cv_small[i].reshape((96,96)),cmap='gist_gray')\n",
    "\n",
    " # Obtaining keypoints positions. x and y coordinates are even and odd indices respectively. \n",
    "    x_points = [predictions[i][j] for j in range(0,30,2)]\n",
    "    y_points = [predictions[i][j+1] for j in range(0,30,2)]\n",
    "      \n",
    "    #plotting keypoints\n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')    \n",
    "  # Including mean keypoints\n",
    "       \n",
    "    ax.plot(mean_x_points, mean_y_points, 'b+', markerfacecolor = 'none')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
