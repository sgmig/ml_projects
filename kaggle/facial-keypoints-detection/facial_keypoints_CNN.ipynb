{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional NN it for facial keypoints recognition\n",
    "\n",
    "Here I will build a convolutional neural network, and train it for the task of facial keypoints recognition. The data are obtained from Kaggle: **LINK**, and consists of **info**.\n",
    "\n",
    "I will build the CNN using tensorflow **link**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading source data.\n",
    "training_data = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images are stored as string. We convert them to a np array. \n",
    "\n",
    "images = training_data['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))\n",
    "images = np.vstack([images.iloc[i] for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating keypoints dataframe. \n",
    "keypoints = training_data.drop('Image', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation: Including reflected pictures into the dataset\n",
    "\n",
    "In order to increase our dataset I will reflect all images left to right, and add them as different images.\n",
    "\n",
    "This implies flipping the images, and reflecting all x-coordinates of the keypoints such that $x_{reflected} = 95 - x_{old}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building an array of reflected images.\n",
    "flipped_ims = np.zeros(images.shape)\n",
    "for j in range(images.shape[0]):\n",
    "    for i in range(96):\n",
    "        flipped_ims[j,i*96:(i+1)*96] = np.flip(images[j,i*96:(i+1)*96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the reflected images to our images array. \n",
    "# RUN THIS CELL ONLY ONCE. \n",
    "images = np.vstack((images, flipped_ims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the keypoints. I need to switch left and right features, \n",
    "# and reflect the x coordinates by x --> 95-x (95 is the last coordinate of the picture. )\n",
    "\n",
    "reflected_keypoints = pd.DataFrame(columns = keypoints.columns)\n",
    "\n",
    "# We look for the correspoding columns by switching 'left' <--> 'right'.\n",
    "for colname in reflected_keypoints.columns:\n",
    "    if 'left' in colname:\n",
    "        reference_col = colname.replace('left', 'right')\n",
    "    elif 'right' in colname:\n",
    "        reference_col = colname.replace('right','left')\n",
    "    else:\n",
    "        reference_col = colname\n",
    "        \n",
    "    # Assigning values and reflecting x coordinates\n",
    "    # reflected_keypoints[colname] = keypoints[reference_col].apply(lambda x: 95-x if colname[-1]=='x' else x)\n",
    "    # the one-line version is fine but I think separating is more readable.\n",
    "    \n",
    "    reflected_keypoints[colname] = keypoints[reference_col]\n",
    "    if colname[-1] == 'x':\n",
    "        reflected_keypoints[colname] = reflected_keypoints[colname].apply(lambda x: 95-x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding reflected keypoints to the original keypoints. \n",
    "# RUN THIS CELL ONLY ONCE.\n",
    "keypoints = pd.concat([keypoints,reflected_keypoints], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a train_test_split in order to have a cross validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cv, keypoints_train, keypoints_cv = train_test_split(images, keypoints, \n",
    "                                                                test_size=500, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the mean poistion the RMSE on the cv set is:  3.8490320110627083\n"
     ]
    }
   ],
   "source": [
    "# BASELINE\n",
    "# As a guide, we compute the error on the cv set obtained by using the mean position of\n",
    "# each keypoint.\n",
    "\n",
    "mean_keypoints = keypoints_train.mean()\n",
    "baseline_rmse = np.sqrt(((keypoints_cv - mean_keypoints)**2).mean().mean())\n",
    "\n",
    "print('Using the mean poistion the RMSE on the cv set is: ', baseline_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good estimation should improve on this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define a function for taking random batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, labels, batch_size):\n",
    "    \"\"\" A function for generating training batches. \n",
    "    X = Collection of examples.\n",
    "    labels = True labels. \n",
    "    batch_size = Number of elements to be randomly selected. \"\"\"\n",
    "    sample_indices = np.random.choice(range(len(X)), size = batch_size, \n",
    "                                      replace = False)\n",
    "    \n",
    "    images = X[sample_indices]\n",
    "    keypoints = labels.iloc[sample_indices]\n",
    "        \n",
    "    return images, keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN\n",
    "\n",
    "I will start by using the same architechture I used in the course.For the moment I will use train_test_split to test my network a bit. Eventually this wont be necessary, as the dataset provides a separate test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers of tensorflow methods. This functions will help make\n",
    "# the construction of the network more straightforward. \n",
    "\n",
    "def init_weights(shape):\n",
    "    #init_random_dist = tf.initializers.random_normal(stddev=0.1)\n",
    "    init_random_dist = tf.initializers.he_normal()\n",
    "    return tf.get_variable('weights', shape=shape ,initializer= init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    #init_bias_vals = tf.random_uniform(shape=shape)\n",
    "    init_bias_vals = tf.initializers.random_uniform()\n",
    "    return tf.get_variable('bias', shape = shape ,initializer= init_bias_vals)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b\n",
    "\n",
    "def output_act_function(input_layer):\n",
    "    factor = tf.constant(96, dtype = tf.float32)\n",
    "    return tf.multiply(factor, tf.nn.sigmoid(input_layer))\n",
    "\n",
    "def sigmoid_layer(input_layer, size, max_val):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    factor = tf.constant(max_val, dtype = tf.float32)\n",
    "    return tf.multiply(factor, tf.nn.sigmoid(tf.matmul(input_layer, W) + b))\n",
    "    \n",
    "\n",
    "def new_bn_layer(input_x, training, decay = 0.999):\n",
    "    \"\"\" Wrapper function for tf.contrib.layers.batch_norm \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.batch_norm(input_x, decay = decay, is_training = training  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handmade implementation of batch_normalization. \n",
    "def init_gamma(shape):\n",
    "    # I am not using the argument shape!\n",
    "    init_gamma_val = tf.constant(1,shape =shape, dtype = tf.float32)\n",
    "    return tf.get_variable('gamma', initializer = init_gamma_val)\n",
    "\n",
    "def init_beta(shape):\n",
    "    # I am not using the argument shape!\n",
    "    init_beta_val = tf.constant(0, shape =shape, dtype = tf.float32)\n",
    "    return tf.get_variable('beta', initializer = init_beta_val)\n",
    "\n",
    "def bn_layer(input_x, is_training, bn_type = 'normal', decay = 0.999):\n",
    "    \n",
    "    \"\"\"Implements batch normalization. \n",
    "    input_x: A Tensor. Is the data to normalize.\n",
    "    is_training: Bool. Indicates whether we are in training o predictino phase. \n",
    "    bn_type: 'normal' or 'convo', indicates over which indeces we take the means. \n",
    "              if 'normal', we use only the first index (batch number).\n",
    "              if 'conv', uses [0,1,2]\n",
    "    decay: float. decay parameter for the running averages. \n",
    "    \"\"\"\n",
    "    \n",
    "    if bn_type == 'normal':\n",
    "        shape = input_x.shape[1:]\n",
    "        axes = [0]\n",
    "    elif bn_type == 'convo':\n",
    "        shape = input_x.shape[-1]\n",
    "        axes = [0,1,2]\n",
    "        \n",
    "    #running mean and variance to be sued for inference.\n",
    "    pop_mean = tf.Variable(tf.zeros(shape), trainable=False) \n",
    "    pop_var = tf.Variable(tf.ones(shape), trainable=False)\n",
    "    \n",
    "    gamma = init_gamma(shape) #in the normalization I am using this is the number of channels. \n",
    "    beta = init_beta(shape)\n",
    "    \n",
    "    batch_mean, batch_var = tf.nn.moments(input_x, axes)  \n",
    "    \n",
    "    def train_phase():\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(input_x, batch_mean, batch_var,\n",
    "                                                    offset = beta, scale = gamma,\n",
    "                                                    variance_epsilon=0.0001)\n",
    "    \n",
    "    def infer_phase(): \n",
    "        return tf.nn.batch_normalization(input_x, pop_mean, pop_var, offset = beta, \n",
    "                                  scale = gamma, variance_epsilon=0.0001)\n",
    "    \n",
    "        \n",
    "    return tf.cond(is_training, train_phase, infer_phase)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDERS\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 9216])\n",
    "keypoints_true = tf.placeholder(tf.float32, [None, 30])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "training = tf.placeholder(tf.bool)\n",
    "#L = tf.placeholder(tf.float32)\n",
    "#drop_rate = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_images = tf.image.per_image_standardization(tf.reshape(x, [-1,96,96,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERS\n",
    "\n",
    "with tf.variable_scope('Convo1'):\n",
    "    convo1 = convolutional_layer(x_images, [6,6,1,32]) # 6 x 6 filter, 1 channel in, 32 channels out. SAME padding.\n",
    "                                            # so output images are also 96 x 96. \n",
    "\n",
    "    convo1_pool = tf.nn.max_pool(convo1, ksize=[1, 4, 4, 1],\n",
    "                          strides=[1, 4, 4, 1], padding='SAME')   #output of 24 x 24 x 32\n",
    "    #convo1_pool_bn = new_bn_layer(convo1_pool, training)\n",
    "    \n",
    "with tf.variable_scope('Convo2'):    \n",
    "    convo2 = convolutional_layer(convo1_pool, [2,2,32,64]) # 4x4 filter, 64 outputs. SAME padding.\n",
    "\n",
    "    convo2_pool = max_pool_2by2(convo2) # 12 x 12 x64\n",
    "    \n",
    "    #convo2_pool_bn = new_bn_layer(convo2_pool, training)\n",
    "        \n",
    "with tf.variable_scope('Convo3'):\n",
    "    convo3 = convolutional_layer(convo2_pool, [2,2,64,128]) # 2x2 filter, 128 outputs. SAME padding.\n",
    "\n",
    "    convo3_pool = max_pool_2by2(convo3) # 6 x 6 x 128\n",
    "\n",
    "    convo3_flat = tf.reshape(convo3_pool,[-1,6*6*128])\n",
    "    \n",
    "with tf.variable_scope('Full_one'):\n",
    "    \n",
    "    full_layer_one = tf.nn.relu(normal_full_layer(convo3_flat,1024))\n",
    "    \n",
    "    full_layer_one_bn = new_bn_layer(full_layer_one, training)\n",
    "    \n",
    "    \n",
    "# DROPOUT \n",
    "\n",
    "#full_one_dropout = tf.nn.dropout(full_layer_one, rate = drop_rate)\n",
    "\n",
    "# OUTPUT LAYER\n",
    "with tf.variable_scope('Output'):\n",
    "    \n",
    "    keypoints_pred = tf.minimum(tf.nn.relu(normal_full_layer(full_layer_one_bn,30)), 96.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# LOSS FUNCTION\n",
    "with tf.variable_scope('Loss'):\n",
    "    masked_loss = tf.reduce_mean(tf.square(\n",
    "        tf.boolean_mask(keypoints_pred - keypoints_true, tf.is_finite(keypoints_true) ) ) ) \n",
    "                                 \n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "        train = optimizer.minimize(masked_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZER\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-df4f1ab248e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         _ , train_loss= sess.run([train, masked_loss], \n\u001b[1;32m     22\u001b[0m                              feed_dict={x: x_batch, keypoints_true: keypoints_batch,\n\u001b[0;32m---> 23\u001b[0;31m                                         lr:0.01, training:True})\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_cnn_\" + str(counter))\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_3\")\n",
    "    train_losses = []\n",
    "    cv_losses = []\n",
    "    \n",
    "    \n",
    "    for iteration in range(num_steps+1):\n",
    "        \n",
    "        x_batch, keypoints_batch = next_batch(x_train, keypoints_train, 32)\n",
    "        \n",
    "        \n",
    "        # Uncomment for BN\n",
    "        _ , train_loss= sess.run([train, masked_loss], \n",
    "                             feed_dict={x: x_batch, keypoints_true: keypoints_batch,\n",
    "                                        lr:0.01, training:True})\n",
    "        \n",
    "        if iteration%100 == 0:\n",
    "            \n",
    "            cv_loss = sess.run(masked_loss,\n",
    "                               feed_dict={x:x_cv, keypoints_true:keypoints_cv, training:False })\n",
    "            \n",
    "            train_losses.append(np.sqrt(train_loss))\n",
    "            cv_losses.append(np.sqrt(cv_loss))\n",
    "                        \n",
    "            # Showing output for tracking progress.\n",
    "            print('=======================================')\n",
    "            print('Currently on step {}'.format(iteration))\n",
    "            print('Train MSE: ', train_losses[-1], '\\n')\n",
    "            print('CV MSE:', cv_losses[-1], '\\n')\n",
    "            \n",
    "            ax.cla()\n",
    "            ax.plot(train_losses[1:], 'r', label = 'Training error')\n",
    "            ax.plot(cv_losses[1:], 'g', label = 'Validation error')\n",
    "            ax.legend()\n",
    "            display(fig)\n",
    "            \n",
    "            print('=======================================')\n",
    "            print('train_losses = ', train_losses, '\\n')\n",
    "            print('cv_losses = ', cv_losses)\n",
    "            \n",
    "            clear_output(wait = True)\n",
    "    \n",
    "    # printing final results\n",
    "    \n",
    "    print('=======================================')\n",
    "    print('Currently on step {}'.format(iteration))\n",
    "    print('Train MSE: ', train_losses[-1], '\\n')\n",
    "    print('CV MSE:',  cv_losses[-1], '\\n')\n",
    "\n",
    "    ax.cla()\n",
    "    ax.plot(train_losses[1:], 'r', label = 'Training error')\n",
    "    ax.plot(cv_losses[1:], 'g', label = 'Validation error')\n",
    "    ax.legend()\n",
    "    \n",
    "            \n",
    "    print('=======================================')\n",
    "    print('train_losses = ', train_losses, '\\n')\n",
    "    print('cv_losses = ', cv_losses)\n",
    "            \n",
    "    saver.save(sess, \"./saved_models/keypoints_cnn_\" + str(counter) )\n",
    "    #saver.save(sess, \"./saved_models/keypoints_cnn_1\"  )\n",
    "    #counter +=1\n",
    "    #print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5462265"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_losses[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.66379"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(cv_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the cv set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Use your Saver instance to restore your saved rnn time series model\n",
    "    saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "\n",
    "    # Create a numpy array for your genreative seed from the last 12 months of the \n",
    "    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n",
    "        \n",
    "    predictions = sess.run(keypoints_pred, feed_dict= {x:x_cv, drop_rate:0 })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index( np.argmax(predictions), predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[73,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 3\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "selection = np.random.choice(range(len(x_cv)), size = grid_size**2, )\n",
    "\n",
    "mean_x_points = [keypoints_mean[j] for j in range(0,30,2)]\n",
    "mean_y_points = [keypoints_mean[j+1] for j in range(0,30,2)]\n",
    "\n",
    "for i, ax in zip(selection, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    # Plotting the faces\n",
    "    ax.imshow(x_cv[i].reshape((96,96)),cmap='gist_gray')\n",
    "\n",
    " # Obtaining keypoints positions. x and y coordinates are even and odd indices respectively. \n",
    "    x_points = [predictions[i][j] for j in range(0,30,2)]\n",
    "    y_points = [predictions[i][j+1] for j in range(0,30,2)]\n",
    "      \n",
    "    #plotting predicted keypoints\n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')    \n",
    "  \n",
    " # Plotting true keypoints\n",
    "    \n",
    "    x_true = [keypoints_cv.iloc[i][j] for j in range(0,30,2)]\n",
    "    y_true = [keypoints_cv.iloc[i][j+1] for j in range(0,30,2)]\n",
    "    \n",
    "    ax.plot(x_true, y_true, 'b*', markerfacecolor = 'none')    \n",
    "    \n",
    " # Including mean keypoints\n",
    "       \n",
    "    #ax.plot(mean_x_points, mean_y_points, 'b+', markerfacecolor = 'none')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_data['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))\n",
    "\n",
    "test_images = np.vstack([test_images.iloc[i] for i in range(len(test_images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Use your Saver instance to restore your saved rnn time series model\n",
    "    saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_3\" )\n",
    "\n",
    "    # Create a numpy array for your genreative seed from the last 12 months of the \n",
    "    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n",
    "        \n",
    "    predictions = sess.run(keypoints_pred, feed_dict= {x:test_images, drop_rate:0 })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 2\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "#selection = np.random.choice(range(len(test_images)), size = grid_size**2 )\n",
    "selection = [131, 159, 491, 525]\n",
    "for i, ax in zip(selection, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    # Plotting the faces\n",
    "    ax.imshow(test_images[i].reshape((96,96)),cmap='gist_gray')\n",
    "\n",
    " # Obtaining keypoints positions. x and y coordinates are even and odd indices respectively. \n",
    "    x_points = [predictions[i][j] for j in range(0,30,2)]\n",
    "    y_points = [predictions[i][j+1] for j in range(0,30,2)]\n",
    "      \n",
    "    #plotting predicted keypoints\n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints = pd.DataFrame(data = predictions, columns = keypoints.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints[(predicted_keypoints>96).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints.to_csv('full_test_predictions_002.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the lookup table to make a submission file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = pd.read_csv('IdLookupTable.csv', header = 0, index_col = 'RowId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locations_list = []\n",
    "for i in lookup_table.iterrows():\n",
    "    position = predicted_keypoints.iloc[i[1]['ImageId']-1][i[1]['FeatureName']]\n",
    "    locations_list.append(position)\n",
    "len(locations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty dataframe for testing submission\n",
    "Submission_df = pd.DataFrame(columns = ['Location'], index = lookup_table.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df['Location'] = locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df.to_csv('submission_002.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
