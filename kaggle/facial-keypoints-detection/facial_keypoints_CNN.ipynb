{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional NN it for facial keypoints recognition\n",
    "\n",
    "Here I will build a convolutional neural network, and train it for the task of facial keypoints recognition. The data are obtained from Kaggle: **LINK**, and consists of **info**.\n",
    "\n",
    "I will build the CNN using tensorflow **link**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading source data.\n",
    "training_data = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images are stored as string. We convert them to a np array. \n",
    "\n",
    "images = training_data['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))\n",
    "images = np.vstack([images.iloc[i] for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating keypoints dataframe. \n",
    "keypoints = training_data.drop('Image', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation: Including reflected pictures into the dataset\n",
    "\n",
    "In order to increase our dataset I will reflect all images left to right, and add them as different images.\n",
    "\n",
    "This implies flipping the images, and reflecting all x-coordinates of the keypoints such that $x_{reflected} = 95 - x_{old}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building an array of reflected images.\n",
    "flipped_ims = np.zeros(images.shape)\n",
    "for j in range(images.shape[0]):\n",
    "    for i in range(96):\n",
    "        flipped_ims[j,i*96:(i+1)*96] = np.flip(images[j,i*96:(i+1)*96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the reflected images to our images array. \n",
    "# RUN THIS CELL ONLY ONCE. \n",
    "images = np.vstack((images, flipped_ims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the keypoints. I need to switch left and right features, \n",
    "# and reflect the x coordinates by x --> 95-x (95 is the last coordinate of the picture. )\n",
    "\n",
    "reflected_keypoints = pd.DataFrame(columns = keypoints.columns)\n",
    "\n",
    "# We look for the correspoding columns by switching 'left' <--> 'right'.\n",
    "for colname in reflected_keypoints.columns:\n",
    "    if 'left' in colname:\n",
    "        reference_col = colname.replace('left', 'right')\n",
    "    elif 'right' in colname:\n",
    "        reference_col = colname.replace('right','left')\n",
    "    else:\n",
    "        reference_col = colname\n",
    "        \n",
    "    # Assigning values and reflecting x coordinates\n",
    "    # reflected_keypoints[colname] = keypoints[reference_col].apply(lambda x: 95-x if colname[-1]=='x' else x)\n",
    "    # the one-line version is fine but I think separating is more readable.\n",
    "    \n",
    "    reflected_keypoints[colname] = keypoints[reference_col]\n",
    "    if colname[-1] == 'x':\n",
    "        reflected_keypoints[colname] = reflected_keypoints[colname].apply(lambda x: 95-x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding reflected keypoints to the original keypoints. \n",
    "# RUN THIS CELL ONLY ONCE.\n",
    "keypoints = pd.concat([keypoints,reflected_keypoints], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a train_test_split in order to have a cross validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Images', images.shape)\n",
    "print('Keypoints', keypoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cv, keypoints_train, keypoints_cv = train_test_split(images, keypoints, \n",
    "                                                                test_size=500, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define a function for taking random batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, labels, batch_size):\n",
    "    \"\"\" A function for generating training batches. \n",
    "    X = Collection of examples.\n",
    "    labels = True labels. \n",
    "    batch_size = Number of elements to be randomly selected. \"\"\"\n",
    "    sample_indices = np.random.choice(range(len(X)), size = batch_size, \n",
    "                                      replace = False)\n",
    "    \n",
    "    images = X[sample_indices]\n",
    "    keypoints = labels.iloc[sample_indices]\n",
    "        \n",
    "    return images, keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN\n",
    "\n",
    "I will start by using the same architechture I used in the course.For the moment I will use train_test_split to test my network a bit. Eventually this wont be necessary, as the dataset provides a separate test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers of tensorflow methods. This functions will help make\n",
    "# the construction of the network more straightforward. \n",
    "\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.initializers.random_normal(stddev=0.1)\n",
    "    return tf.get_variable('weights', shape=shape ,initializer= init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    #init_bias_vals = tf.random_uniform(shape=shape)\n",
    "    init_bias_vals = tf.initializers.random_uniform()\n",
    "    return tf.get_variable('bias', shape = shape ,initializer= init_bias_vals)\n",
    "\n",
    "def init_gamma(shape):\n",
    "    # I am not using the argument shape!\n",
    "    init_gamma_val = tf.constant(1, dtype = tf.float32)\n",
    "    return tf.get_variable('gamma', intializer = init_gamma_val)\n",
    "\n",
    "def init_beta(shape):\n",
    "    # I am not using the argument shape!\n",
    "    init_beta_val = tf.constant(0, dtype = tf.float32)\n",
    "    return tf.get_variable('beta', initializer = init_beta_val)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 4, 4, 1],\n",
    "                          strides=[1, 4, 4, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b\n",
    "\n",
    "def output_act_function(input_layer):\n",
    "    factor = tf.constant(96, dtype = tf.float32)\n",
    "    return tf.multiply(factor, tf.nn.sigmoid(input_layer))\n",
    "\n",
    "def sigmoid_layer(input_layer, size, max_val):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    factor = tf.constant(max_val, dtype = tf.float32)\n",
    "    return tf.multiply(factor, tf.nn.sigmoid(tf.matmul(input_layer, W) + b))\n",
    "    \n",
    "\n",
    "def bn_layer(input_x, is_training, bn_type = 'normal', decay = 0.999):\n",
    "    \n",
    "    \"\"\"Implements batch normalization. \n",
    "    input_x: A Tensor. Is the data to normalize.\n",
    "    is_training: Bool. Indicates whether we are in training o predictino phase. \n",
    "    bn_type: 'normal' or 'convo', indicates over which indeces we take the means. \n",
    "              if 'normal', we use only the first index (batch number).\n",
    "              if 'conv', uses [0,1,2]\n",
    "    decay: float. decay parameter for the running averages. \n",
    "    \"\"\"\n",
    "    \n",
    "    if bn_type == 'normal':\n",
    "        shape = input_x.shape[1:]\n",
    "        axes = [0]\n",
    "    elif bn_type == 'convo':\n",
    "        shape = input_x.shape[-1]\n",
    "        axes = [0,1,2]\n",
    "        \n",
    "    #running mean and variance to be sued for inference.\n",
    "    pop_mean = tf.Variable(tf.zeros(shape), trainable=False) \n",
    "    pop_var = tf.Variable(tf.ones(shape), trainable=False)\n",
    "    \n",
    "    gamma = init_gamma(shape) #in the normalization I am using this is the number of channels. \n",
    "    beta = init_beta(shape)\n",
    "    \n",
    "    batch_mean, batch_var = tf.nn.moments(input_x, axes)  \n",
    "    \n",
    "    def train_phase():\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(input_x, batch_mean, batch_var,\n",
    "                                                    offset = beta, scale = gamma,\n",
    "                                                    variance_epsilon=0.0001)\n",
    "    \n",
    "    def infer_phase(): \n",
    "        return tf.nn.batch_normalization(input_x, pop_mean, pop_var, offset = beta, \n",
    "                                  scale = gamma, variance_epsilon=0.0001)\n",
    "    \n",
    "        \n",
    "    return tf.cond(is_training, train_phase, infer_phase)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = init_weights([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "Weights_2by2 = sess.run(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights_2by2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo1 = convolutional_layer(x_images, [6,6,1,32]) # 6 x 6 filter, 1 channel in, 32 channels out. SAME padding.\n",
    "                                            # so output images are also 96 x 96. \n",
    "\n",
    "convo1_pool = max_pool_2by2(convo1)   #output of 24 x 24 x 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDERS\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 9216])\n",
    "keypoints_true = tf.placeholder(tf.float32, [None, 30])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "training = tf.placeholder(tf.bool)\n",
    "drop_rate = tf.placeholder(tf.float32)\n",
    "L = tf.placeholder(tf.float32) # multiplier for the regularization term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_images = tf.image.per_image_standardization(tf.reshape(x, [-1,96,96,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERS\n",
    "\n",
    "convo1 = convolutional_layer(x_images, [6,6,1,32]) # 6 x 6 filter, 1 channel in, 32 channels out. SAME padding.\n",
    "                                            # so output images are also 96 x 96. \n",
    "\n",
    "convo1_pool = max_pool_2by2(convo1)   #output of 24 x 24 x 32\n",
    "\n",
    "convo2 = convolutional_layer(convo1_pool, [2,2,32,64]) # 4x4 filter, 64 outputs. SAME padding.\n",
    "\n",
    "convo2_pool = tf.nn.max_pool(convo2, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME') # 12 x 12 x64\n",
    "\n",
    "convo3 = convolutional_layer(convo2_pool, [2,2,64,128]) # 2x2 filter, 128 outputs. SAME padding.\n",
    "\n",
    "convo3_pool = tf.nn.max_pool(convo3, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME') # 6 x 6 x 128\n",
    "\n",
    "\n",
    "convo3_flat = tf.reshape(convo3_pool,[-1,6*6*128])\n",
    "\n",
    "flat_bn = bn_layer(normal_full_layer(convo3_flat,1024), is_training = training)\n",
    "\n",
    "full_layer_one = tf.nn.relu(flat_bn)\n",
    "\n",
    "# DROPOUT AND OUTPOUT LAYER\n",
    "\n",
    "#full_one_dropout = tf.nn.dropout(full_layer_one, rate = drop_rate)\n",
    "\n",
    "full_layer_two = normal_full_layer(full_layer_one, 30)\n",
    "\n",
    "full_bn = bn_layer(full_layer_two, is_training = training )\n",
    "\n",
    "#keypoints_pred = tf.nn.sigmoid(full_bn)\n",
    "keypoints_pred = tf.math.minimum(tf.nn.relu(full_bn), 96.0)\n",
    "\n",
    "#keypoints_pred = tf.nn.relu(normal_full_layer(full_one_dropout,30))\n",
    "#keypoints_pred = normal_full_layer(full_one_dropout,30)\n",
    "#added output function to keep outputs below 96\n",
    "\n",
    "#keypoints_pred = sigmoid_layer(full_one_dropout, 30, 96)\n",
    "\n",
    "#pre_pred_bn = bn_layer(full_layer_one, is_training =training)\n",
    "#keypoints_pred = normal_full_layer(pre_pred_bn,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "masked_loss = tf.reduce_mean(tf.square( \n",
    "    tf.boolean_mask(keypoints_pred - keypoints_true,\n",
    "                        tf.is_finite(keypoints_true) )\n",
    "    ))\n",
    "                                 \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train = optimizer.minimize(masked_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZER\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_3\")\n",
    "    train_losses = []\n",
    "    cv_losses = []\n",
    "    \n",
    "    \n",
    "    for iteration in range(num_steps+1):\n",
    "        \n",
    "        x_batch, keypoints_batch = next_batch(x_train, keypoints_train, 32)\n",
    "        \n",
    "        _ , train_loss= sess.run([train, masked_loss], \n",
    "                                  feed_dict={x: x_batch, keypoints_true: keypoints_batch,\n",
    "                                             lr:0.1, training:True})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        \n",
    "        #if (iteration%20 == 0) and (iteration >0):\n",
    "            #print(':')\n",
    "        \n",
    "        if iteration%100 == 0:\n",
    "            \n",
    "            cv_loss = sess.run(masked_loss, feed_dict={x:x_cv,keypoints_true:keypoints_cv, \n",
    "                                                      training: False})\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            cv_losses.append(cv_loss)\n",
    "                        \n",
    "            # Showing output for tracking progress.\n",
    "            print('=======================================')\n",
    "            print('Currently on step {}'.format(iteration))\n",
    "            print('Train MSE: ', train_loss, '\\n')\n",
    "            print('CV MSE:', cv_loss, '\\n')\n",
    "            \n",
    "            ax.cla()\n",
    "            ax.plot(train_losses[1:], 'r', label = 'Training error')\n",
    "            ax.plot(cv_losses[1:], 'g', label = 'Validation error')\n",
    "            ax.legend()\n",
    "            display(fig)\n",
    "            \n",
    "            print('=======================================')\n",
    "            print('train_losses = ', train_losses, '\\n')\n",
    "            print('cv_losses = ', cv_losses)\n",
    "            \n",
    "            clear_output(wait = True)\n",
    "    \n",
    "    # printing final results\n",
    "    \n",
    "    print('=======================================')\n",
    "    print('Currently on step {}'.format(iteration))\n",
    "    print('Train MSE: ', train_loss, '\\n')\n",
    "    print('CV MSE:', cv_loss, '\\n')\n",
    "\n",
    "    ax.cla()\n",
    "    ax.plot(train_losses[1:], 'r', label = 'Training error')\n",
    "    ax.plot(cv_losses[1:], 'g', label = 'Validation error')\n",
    "    ax.legend()\n",
    "    \n",
    "            \n",
    "    print('=======================================')\n",
    "    print('train_losses = ', train_losses, '\\n')\n",
    "    print('cv_losses = ', cv_losses)\n",
    "            \n",
    "    #saver.save(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter) )\n",
    "    #saver.save(sess, \"./saved_models/keypoints_3_lay_cnn_1\"  )\n",
    "    #counter +=1\n",
    "    #print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(cv_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the cv set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Use your Saver instance to restore your saved rnn time series model\n",
    "    saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "\n",
    "    # Create a numpy array for your genreative seed from the last 12 months of the \n",
    "    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n",
    "        \n",
    "    predictions = sess.run(keypoints_pred, feed_dict= {x:x_cv, drop_rate:0 })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index( np.argmax(predictions), predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[73,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 3\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "selection = np.random.choice(range(len(x_cv)), size = grid_size**2, )\n",
    "\n",
    "mean_x_points = [keypoints_mean[j] for j in range(0,30,2)]\n",
    "mean_y_points = [keypoints_mean[j+1] for j in range(0,30,2)]\n",
    "\n",
    "for i, ax in zip(selection, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    # Plotting the faces\n",
    "    ax.imshow(x_cv[i].reshape((96,96)),cmap='gist_gray')\n",
    "\n",
    " # Obtaining keypoints positions. x and y coordinates are even and odd indices respectively. \n",
    "    x_points = [predictions[i][j] for j in range(0,30,2)]\n",
    "    y_points = [predictions[i][j+1] for j in range(0,30,2)]\n",
    "      \n",
    "    #plotting predicted keypoints\n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')    \n",
    "  \n",
    " # Plotting true keypoints\n",
    "    \n",
    "    x_true = [keypoints_cv.iloc[i][j] for j in range(0,30,2)]\n",
    "    y_true = [keypoints_cv.iloc[i][j+1] for j in range(0,30,2)]\n",
    "    \n",
    "    ax.plot(x_true, y_true, 'b*', markerfacecolor = 'none')    \n",
    "    \n",
    " # Including mean keypoints\n",
    "       \n",
    "    #ax.plot(mean_x_points, mean_y_points, 'b+', markerfacecolor = 'none')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_data['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))\n",
    "\n",
    "test_images = np.vstack([test_images.iloc[i] for i in range(len(test_images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Use your Saver instance to restore your saved rnn time series model\n",
    "    saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_3\" )\n",
    "\n",
    "    # Create a numpy array for your genreative seed from the last 12 months of the \n",
    "    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n",
    "        \n",
    "    predictions = sess.run(keypoints_pred, feed_dict= {x:test_images, drop_rate:0 })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 2\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "#selection = np.random.choice(range(len(test_images)), size = grid_size**2 )\n",
    "selection = [131, 159, 491, 525]\n",
    "for i, ax in zip(selection, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    # Plotting the faces\n",
    "    ax.imshow(test_images[i].reshape((96,96)),cmap='gist_gray')\n",
    "\n",
    " # Obtaining keypoints positions. x and y coordinates are even and odd indices respectively. \n",
    "    x_points = [predictions[i][j] for j in range(0,30,2)]\n",
    "    y_points = [predictions[i][j+1] for j in range(0,30,2)]\n",
    "      \n",
    "    #plotting predicted keypoints\n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints = pd.DataFrame(data = predictions, columns = keypoints.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints[(predicted_keypoints>96).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints.to_csv('full_test_predictions_002.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the lookup table to make a submission file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = pd.read_csv('IdLookupTable.csv', header = 0, index_col = 'RowId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locations_list = []\n",
    "for i in lookup_table.iterrows():\n",
    "    position = predicted_keypoints.iloc[i[1]['ImageId']-1][i[1]['FeatureName']]\n",
    "    locations_list.append(position)\n",
    "len(locations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty dataframe for testing submission\n",
    "Submission_df = pd.DataFrame(columns = ['Location'], index = lookup_table.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df['Location'] = locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df.to_csv('submission_002.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
