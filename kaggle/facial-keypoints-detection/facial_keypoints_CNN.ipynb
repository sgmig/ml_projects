{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional NN it for facial keypoints recognition\n",
    "\n",
    "Here I will build a convolutional neural network, and train it for the task of facial keypoints recognition. The data are obtained from Kaggle: **LINK**, and consists of **info**.\n",
    "\n",
    "I will build the CNN using tensorflow **link**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading source data.\n",
    "training_data = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images are stored as string. We convert them to a np array. \n",
    "\n",
    "images = training_data['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))\n",
    "images = np.vstack([images.iloc[i] for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating keypoints dataframe. \n",
    "keypoints = training_data.drop('Image', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation: Including reflected pictures into the dataset\n",
    "\n",
    "In order to increase our dataset I will reflect all images left to right, and add them as different images.\n",
    "\n",
    "This implies flipping the images, and reflecting all x-coordinates of the keypoints such that $x_{reflected} = 95 - x_{old}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building an array of reflected images.\n",
    "flipped_ims = np.zeros(images.shape)\n",
    "for j in range(images.shape[0]):\n",
    "    for i in range(96):\n",
    "        flipped_ims[j,i*96:(i+1)*96] = np.flip(images[j,i*96:(i+1)*96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the reflected images to our images array. \n",
    "# RUN THIS CELL ONLY ONCE. \n",
    "images = np.vstack((images, flipped_ims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the keypoints. I need to switch left and right features, \n",
    "# and reflect the x coordinates by x --> 95-x (95 is the last coordinate of the picture. )\n",
    "\n",
    "reflected_keypoints = pd.DataFrame(columns = keypoints.columns)\n",
    "\n",
    "# We look for the correspoding columns by switching 'left' <--> 'right'.\n",
    "for colname in reflected_keypoints.columns:\n",
    "    if 'left' in colname:\n",
    "        reference_col = colname.replace('left', 'right')\n",
    "    elif 'right' in colname:\n",
    "        reference_col = colname.replace('right','left')\n",
    "    else:\n",
    "        reference_col = colname\n",
    "        \n",
    "    # Assigning values and reflecting x coordinates\n",
    "    # reflected_keypoints[colname] = keypoints[reference_col].apply(lambda x: 95-x if colname[-1]=='x' else x)\n",
    "    # the one-line version is fine but I think separating is more readable.\n",
    "    \n",
    "    reflected_keypoints[colname] = keypoints[reference_col]\n",
    "    if colname[-1] == 'x':\n",
    "        reflected_keypoints[colname] = reflected_keypoints[colname].apply(lambda x: 95-x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding reflected keypoints to the original keypoints. \n",
    "# RUN THIS CELL ONLY ONCE.\n",
    "keypoints = pd.concat([keypoints,reflected_keypoints], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a train_test_split in order to have a cross validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cv, keypoints_train, keypoints_cv = train_test_split(images, keypoints, \n",
    "                                                                test_size=500, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the mean poistion the RMSE on the cv set is:  3.8490320110627083\n"
     ]
    }
   ],
   "source": [
    "# BASELINE\n",
    "# As a guide, we compute the error on the cv set obtained by using the mean position of\n",
    "# each keypoint.\n",
    "\n",
    "mean_keypoints = keypoints_train.mean()\n",
    "baseline_rmse = np.sqrt(((keypoints_cv - mean_keypoints)**2).mean().mean())\n",
    "\n",
    "print('Using the mean poistion the RMSE on the cv set is: ', baseline_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good estimation should improve on this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define a function for taking random batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, labels, batch_size):\n",
    "    \"\"\" A function for generating training batches. \n",
    "    X = Collection of examples.\n",
    "    labels = True labels. \n",
    "    batch_size = Number of elements to be randomly selected. \"\"\"\n",
    "    sample_indices = np.random.choice(range(len(X)), size = batch_size, \n",
    "                                      replace = False)\n",
    "    \n",
    "    images = X[sample_indices]\n",
    "    keypoints = labels.iloc[sample_indices]\n",
    "        \n",
    "    return images, keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN\n",
    "\n",
    "I will start by using the same architechture I used in the course.For the moment I will use train_test_split to test my network a bit. Eventually this wont be necessary, as the dataset provides a separate test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers of tensorflow methods. This functions will help make\n",
    "# the construction of the network more straightforward. \n",
    "\n",
    "def init_weights(shape):\n",
    "    #init_random_dist = tf.initializers.random_normal(stddev=0.1)\n",
    "    init_random_dist = tf.initializers.he_normal()\n",
    "    return tf.get_variable('weights', shape=shape ,initializer= init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    #init_bias_vals = tf.random_uniform(shape=shape)\n",
    "    init_bias_vals = tf.initializers.random_uniform()\n",
    "    return tf.get_variable('bias', shape = shape ,initializer= init_bias_vals)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b\n",
    "\n",
    "def output_act_function(input_layer):\n",
    "    factor = tf.constant(96, dtype = tf.float32)\n",
    "    return tf.multiply(factor, tf.nn.sigmoid(input_layer))\n",
    "\n",
    "def sigmoid_layer(input_layer, size, max_val):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    factor = tf.constant(max_val, dtype = tf.float32)\n",
    "    return tf.multiply(factor, tf.nn.sigmoid(tf.matmul(input_layer, W) + b))\n",
    "    \n",
    "\n",
    "def new_bn_layer(input_x, training, decay = 0.999):\n",
    "    \"\"\" Wrapper function for tf.contrib.layers.batch_norm \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.batch_norm(input_x, decay = decay, is_training = training  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handmade implementation of batch_normalization. \n",
    "def init_gamma(shape):\n",
    "    # I am not using the argument shape!\n",
    "    init_gamma_val = tf.constant(1,shape =shape, dtype = tf.float32)\n",
    "    return tf.get_variable('gamma', initializer = init_gamma_val)\n",
    "\n",
    "def init_beta(shape):\n",
    "    # I am not using the argument shape!\n",
    "    init_beta_val = tf.constant(0, shape =shape, dtype = tf.float32)\n",
    "    return tf.get_variable('beta', initializer = init_beta_val)\n",
    "\n",
    "def bn_layer(input_x, is_training, bn_type = 'normal', decay = 0.999):\n",
    "    \n",
    "    \"\"\"Implements batch normalization. \n",
    "    input_x: A Tensor. Is the data to normalize.\n",
    "    is_training: Bool. Indicates whether we are in training o predictino phase. \n",
    "    bn_type: 'normal' or 'convo', indicates over which indeces we take the means. \n",
    "              if 'normal', we use only the first index (batch number).\n",
    "              if 'conv', uses [0,1,2]\n",
    "    decay: float. decay parameter for the running averages. \n",
    "    \"\"\"\n",
    "    \n",
    "    if bn_type == 'normal':\n",
    "        shape = input_x.shape[1:]\n",
    "        axes = [0]\n",
    "    elif bn_type == 'convo':\n",
    "        shape = input_x.shape[-1]\n",
    "        axes = [0,1,2]\n",
    "        \n",
    "    #running mean and variance to be sued for inference.\n",
    "    pop_mean = tf.Variable(tf.zeros(shape), trainable=False) \n",
    "    pop_var = tf.Variable(tf.ones(shape), trainable=False)\n",
    "    \n",
    "    gamma = init_gamma(shape) #in the normalization I am using this is the number of channels. \n",
    "    beta = init_beta(shape)\n",
    "    \n",
    "    batch_mean, batch_var = tf.nn.moments(input_x, axes)  \n",
    "    \n",
    "    def train_phase():\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(input_x, batch_mean, batch_var,\n",
    "                                                    offset = beta, scale = gamma,\n",
    "                                                    variance_epsilon=0.0001)\n",
    "    \n",
    "    def infer_phase(): \n",
    "        return tf.nn.batch_normalization(input_x, pop_mean, pop_var, offset = beta, \n",
    "                                  scale = gamma, variance_epsilon=0.0001)\n",
    "    \n",
    "        \n",
    "    return tf.cond(is_training, train_phase, infer_phase)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDERS\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 9216])\n",
    "keypoints_true = tf.placeholder(tf.float32, [None, 30])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "training = tf.placeholder(tf.bool)\n",
    "#drop_rate = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_images = tf.image.per_image_standardization(tf.reshape(x, [-1,96,96,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERS\n",
    "\n",
    "with tf.variable_scope('Convo1'):\n",
    "    convo1 = convolutional_layer(x_images, [6,6,1,32]) # 6 x 6 filter, 1 channel in, 32 channels out. SAME padding.\n",
    "                                            # so output images are also 96 x 96. \n",
    "\n",
    "    convo1_pool = tf.nn.max_pool(convo1, ksize=[1, 4, 4, 1],\n",
    "                          strides=[1, 4, 4, 1], padding='SAME')   #output of 24 x 24 x 32\n",
    "    convo1_pool_bn = new_bn_layer(convo1_pool, training)\n",
    "    \n",
    "with tf.variable_scope('Convo2'):    \n",
    "    convo2 = convolutional_layer(convo1_pool, [2,2,32,64]) # 4x4 filter, 64 outputs. SAME padding.\n",
    "\n",
    "    convo2_pool = max_pool_2by2(convo2) # 12 x 12 x64\n",
    "    \n",
    "    convo2_pool_bn = new_bn_layer(convo2_pool, training)\n",
    "        \n",
    "with tf.variable_scope('Convo3'):\n",
    "    convo3 = convolutional_layer(convo2_pool_bn, [2,2,64,128]) # 2x2 filter, 128 outputs. SAME padding.\n",
    "\n",
    "    convo3_pool = max_pool_2by2(convo3) # 6 x 6 x 128\n",
    "\n",
    "    convo3_flat = tf.reshape(convo3_pool,[-1,6*6*128])\n",
    "    \n",
    "with tf.variable_scope('Full_one'):\n",
    "    \n",
    "    full_layer_one = tf.nn.relu(normal_full_layer(convo3_flat,1024))\n",
    "    \n",
    "    \n",
    "# DROPOUT \n",
    "\n",
    "#full_one_dropout = tf.nn.dropout(full_layer_one, rate = drop_rate)\n",
    "\n",
    "# OUTPUT LAYER\n",
    "with tf.variable_scope('Output'):\n",
    "    \n",
    "    keypoints_pred = normal_full_layer(full_layer_one,30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# LOSS FUNCTION\n",
    "with tf.variable_scope('Loss'):\n",
    "    masked_loss = tf.reduce_mean(tf.square(\n",
    "        tf.boolean_mask(keypoints_pred - keypoints_true, tf.is_finite(keypoints_true) ) ) ) \n",
    "                                 \n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "        train = optimizer.minimize(masked_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZER\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Currently on step 5000\n",
      "Train MSE:  2.1492991 \n",
      "\n",
      "CV MSE: 3.3283215 \n",
      "\n",
      "=======================================\n",
      "train_losses =  [52.65591, 4.3662996, 5.531327, 4.241284, 3.431813, 3.8656058, 4.520331, 3.150718, 3.5821953, 3.438073, 2.5722547, 3.1375036, 2.4980402, 2.272494, 2.9688249, 2.6699834, 3.3613229, 2.9772696, 2.6993725, 2.5843358, 2.431138, 2.6904876, 8.208358, 3.23736, 2.871457, 2.991215, 3.1779091, 2.574399, 2.8748984, 4.3870935, 2.4439566, 3.6666698, 2.3058314, 2.8013597, 2.8830214, 5.8096747, 2.789279, 3.3037024, 5.103158, 3.302552, 2.8730068, 2.7034433, 4.4270926, 3.9971626, 3.1941097, 2.8470063, 2.5778532, 3.2256682, 2.434972, 2.2919488, 2.1492991] \n",
      "\n",
      "cv_losses =  [802.39435, 50.510868, 19.69572, 8.496447, 5.3034887, 10.053642, 10.9338045, 10.476763, 10.1021, 10.15487, 9.503062, 7.875655, 7.239642, 7.9763975, 7.8327527, 4.7128506, 4.2126164, 5.8639283, 4.4445877, 3.9935384, 3.9332688, 4.494572, 3.665673, 3.282749, 3.331512, 3.2343357, 3.6403701, 3.1758094, 3.3201516, 3.247469, 3.7054958, 3.2273803, 3.1696033, 3.2986488, 3.1933422, 3.2222893, 3.6488166, 3.309267, 4.552692, 3.594504, 3.4952295, 3.7265866, 3.5207183, 4.2065606, 4.286511, 3.3241115, 3.0801556, 3.411106, 3.1979172, 3.3802743, 3.3283215]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_3\")\n",
    "    train_losses = []\n",
    "    cv_losses = []\n",
    "    \n",
    "    \n",
    "    for iteration in range(num_steps+1):\n",
    "        \n",
    "        x_batch, keypoints_batch = next_batch(x_train, keypoints_train, 32)\n",
    "        \n",
    "        \n",
    "        # Uncomment for BN\n",
    "        _ , train_loss= sess.run([train, masked_loss], \n",
    "                             feed_dict={x: x_batch, keypoints_true: keypoints_batch,\n",
    "                                        lr:0.01, training:True})\n",
    "        \n",
    "        if iteration%100 == 0:\n",
    "            \n",
    "            cv_loss = sess.run(masked_loss, feed_dict={x:x_cv,keypoints_true:keypoints_cv,\n",
    "                                                       training:False})\n",
    "            \n",
    "            train_losses.append(np.sqrt(train_loss))\n",
    "            cv_losses.append(np.sqrt(cv_loss))\n",
    "                        \n",
    "            # Showing output for tracking progress.\n",
    "            print('=======================================')\n",
    "            print('Currently on step {}'.format(iteration))\n",
    "            print('Train MSE: ', train_losses[-1], '\\n')\n",
    "            print('CV MSE:', cv_losses[-1], '\\n')\n",
    "            \n",
    "            ax.cla()\n",
    "            ax.plot(train_losses[1:], 'r', label = 'Training error')\n",
    "            ax.plot(cv_losses[1:], 'g', label = 'Validation error')\n",
    "            ax.legend()\n",
    "            display(fig)\n",
    "            \n",
    "            print('=======================================')\n",
    "            print('train_losses = ', train_losses, '\\n')\n",
    "            print('cv_losses = ', cv_losses)\n",
    "            \n",
    "            clear_output(wait = True)\n",
    "    \n",
    "    # printing final results\n",
    "    \n",
    "    print('=======================================')\n",
    "    print('Currently on step {}'.format(iteration))\n",
    "    print('Train MSE: ', train_losses[-1], '\\n')\n",
    "    print('CV MSE:',  cv_losses[-1], '\\n')\n",
    "\n",
    "    ax.cla()\n",
    "    ax.plot(train_losses[1:], 'r', label = 'Training error')\n",
    "    ax.plot(cv_losses[1:], 'g', label = 'Validation error')\n",
    "    ax.legend()\n",
    "    \n",
    "            \n",
    "    print('=======================================')\n",
    "    print('train_losses = ', train_losses, '\\n')\n",
    "    print('cv_losses = ', cv_losses)\n",
    "            \n",
    "    #saver.save(sess, \"./saved_models/keypoints_cnn_\" + str(counter) )\n",
    "    #saver.save(sess, \"./saved_models/keypoints_3_lay_cnn_1\"  )\n",
    "    #counter +=1\n",
    "    #print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5462265"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cv_losses[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.66379"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(cv_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the cv set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Use your Saver instance to restore your saved rnn time series model\n",
    "    saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "\n",
    "    # Create a numpy array for your genreative seed from the last 12 months of the \n",
    "    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n",
    "        \n",
    "    predictions = sess.run(keypoints_pred, feed_dict= {x:x_cv, drop_rate:0 })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index( np.argmax(predictions), predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[73,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 3\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "selection = np.random.choice(range(len(x_cv)), size = grid_size**2, )\n",
    "\n",
    "mean_x_points = [keypoints_mean[j] for j in range(0,30,2)]\n",
    "mean_y_points = [keypoints_mean[j+1] for j in range(0,30,2)]\n",
    "\n",
    "for i, ax in zip(selection, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    # Plotting the faces\n",
    "    ax.imshow(x_cv[i].reshape((96,96)),cmap='gist_gray')\n",
    "\n",
    " # Obtaining keypoints positions. x and y coordinates are even and odd indices respectively. \n",
    "    x_points = [predictions[i][j] for j in range(0,30,2)]\n",
    "    y_points = [predictions[i][j+1] for j in range(0,30,2)]\n",
    "      \n",
    "    #plotting predicted keypoints\n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')    \n",
    "  \n",
    " # Plotting true keypoints\n",
    "    \n",
    "    x_true = [keypoints_cv.iloc[i][j] for j in range(0,30,2)]\n",
    "    y_true = [keypoints_cv.iloc[i][j+1] for j in range(0,30,2)]\n",
    "    \n",
    "    ax.plot(x_true, y_true, 'b*', markerfacecolor = 'none')    \n",
    "    \n",
    " # Including mean keypoints\n",
    "       \n",
    "    #ax.plot(mean_x_points, mean_y_points, 'b+', markerfacecolor = 'none')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_data['Image'].apply(lambda str_pic: np.array([int(px) for px in str_pic.split()]))\n",
    "\n",
    "test_images = np.vstack([test_images.iloc[i] for i in range(len(test_images))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Use your Saver instance to restore your saved rnn time series model\n",
    "    saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_\" + str(counter))\n",
    "    #saver.restore(sess, \"./saved_models/keypoints_3_lay_cnn_3\" )\n",
    "\n",
    "    # Create a numpy array for your genreative seed from the last 12 months of the \n",
    "    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n",
    "        \n",
    "    predictions = sess.run(keypoints_pred, feed_dict= {x:test_images, drop_rate:0 })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 2\n",
    "fig, axes = plt.subplots(grid_size, grid_size, gridspec_kw = dict(hspace = .05, wspace = .05), \n",
    "                         figsize=(10,10))\n",
    "\n",
    "#selection = np.random.choice(range(len(test_images)), size = grid_size**2 )\n",
    "selection = [131, 159, 491, 525]\n",
    "for i, ax in zip(selection, axes.flat):\n",
    "\n",
    "    ax.axis('off')\n",
    "    # Plotting the faces\n",
    "    ax.imshow(test_images[i].reshape((96,96)),cmap='gist_gray')\n",
    "\n",
    " # Obtaining keypoints positions. x and y coordinates are even and odd indices respectively. \n",
    "    x_points = [predictions[i][j] for j in range(0,30,2)]\n",
    "    y_points = [predictions[i][j+1] for j in range(0,30,2)]\n",
    "      \n",
    "    #plotting predicted keypoints\n",
    "    ax.plot(x_points, y_points, 'ro', markerfacecolor = 'none')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints = pd.DataFrame(data = predictions, columns = keypoints.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints[(predicted_keypoints>96).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints.to_csv('full_test_predictions_002.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the lookup table to make a submission file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = pd.read_csv('IdLookupTable.csv', header = 0, index_col = 'RowId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locations_list = []\n",
    "for i in lookup_table.iterrows():\n",
    "    position = predicted_keypoints.iloc[i[1]['ImageId']-1][i[1]['FeatureName']]\n",
    "    locations_list.append(position)\n",
    "len(locations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty dataframe for testing submission\n",
    "Submission_df = pd.DataFrame(columns = ['Location'], index = lookup_table.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df['Location'] = locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df.to_csv('submission_002.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
